{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United States Census Bureau's 2017 Basic Monthly CPS, using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEPS 1 - 2 :** Download the DOS/Windows for Dec. zip file, extract file.\n",
    "+ Visit https://www.census.gov/data/datasets/2017/demo/cps/cps-basic-2017.html to download the zip file.\n",
    "+ The downloaded file *dec17pub.zip*,is  available in the project root folder.\n",
    "+ If not extracted, it will be extracted into the root folder\n",
    "+ Load essential pyspark libraries and initialize spark context and session.\n",
    "+ Display spark engine version as a validation for an active session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dec17pub.dat has been extracted.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import zipfile\n",
    "\n",
    "zip_file = pathlib.Path('dec17pub.zip')\n",
    "file_path = pathlib.Path('dec17pub.dat')\n",
    "file_name = 'dec17pub.dat'\n",
    "\n",
    "if not file_path.is_file():\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        # Check if the file exists in the ZIP archive\n",
    "        if file_name in zip_ref.namelist():\n",
    "            # Extract the file to a current location\n",
    "            zip_ref.extract(file_name)\n",
    "            print(f'\\n{file_name} has been extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The spark version is : 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, format_number\n",
    "\n",
    "spark = SparkSession.builder.appName('Data Engineer - Take Home Project').getOrCreate()\n",
    "print(f'\\nThe spark version is : {spark.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 3 :** Showing a sample of *DOS/Windows for Dec* zip file\n",
    "+ Open the file and map columns from each line to corresponding variables.\n",
    "+ Add each mapped line to a List.\n",
    "+ Create a dataframe *df_Master* from the List.\n",
    "+ Display sample records from the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A Sample Record from dec17pub.dat\n",
      "+-------------------------+-----------------+----------------------------+-------------------------+-------------------+----------------------------+-----------------------------------+--------------------------------------+----------------------+------------------------+----------------------+---------+\n",
      "|full_household_identifier|time_of_interview|final_outcome_of_survey_code|type_of_housing_unit_code|household_type_code|household_has_telephone_code|household_can_access_telephone_code|is_telephone_interview_acceptable_code|type_of_interview_code|family_income_range_code|division_location_code|race_code|\n",
      "+-------------------------+-----------------+----------------------------+-------------------------+-------------------+----------------------------+-----------------------------------+--------------------------------------+----------------------+------------------------+----------------------+---------+\n",
      "|          000004795110719|          2017/12|                         201|                        1|                  1|                           1|                                  1|                                     1|                     2|                       9|                     6|        1|\n",
      "+-------------------------+-----------------+----------------------------+-------------------------+-------------------+----------------------------+-----------------------------------+--------------------------------------+----------------------+------------------------+----------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create list to hold records from file\n",
    "rows = list()\n",
    "\n",
    "# Open the file for read operation\n",
    "with open('dec17pub.dat','r') as file:\n",
    "    \n",
    "    # Read each line and map the columns to variables\n",
    "    for line in file:\n",
    "        full_household_identifier = line[:15]\n",
    "        time_of_interview = line[17:21] + '/' + line[15:17]\n",
    "        final_outcome_of_survey = line[23:26]\n",
    "        type_of_housing_unit = line[31:32]\n",
    "        household_type = line[61:62]\n",
    "        household_has_telephone = line[33:34]\n",
    "        household_can_access_telephone = line[35:36]\n",
    "        is_telephone_interview_acceptable = line[37:38]\n",
    "        type_of_interview = line[65:66]\n",
    "        family_income_range = line[39:40]\n",
    "        division_location =  line[90:91]\n",
    "        race =  line[138:140]\n",
    "\n",
    "        #Create a record from above variables and add record to a List\n",
    "        item = (full_household_identifier,\n",
    "                time_of_interview,\n",
    "                final_outcome_of_survey,\n",
    "                type_of_housing_unit,\n",
    "                household_type,\n",
    "                household_has_telephone,\n",
    "                household_can_access_telephone,\n",
    "                is_telephone_interview_acceptable,\n",
    "                type_of_interview,\n",
    "                family_income_range,    \n",
    "                division_location,\n",
    "                race)\n",
    "        rows.append(item)\n",
    "\n",
    "# Create a dataframe and display 3 sample records\n",
    "from schema import master_schema\n",
    "df_master = spark.createDataFrame(rows, master_schema)\n",
    "\n",
    "print(f'\\n\\nA Sample Record from {file_name}')\n",
    "df_master.show(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 4 :** Answer to Questions 1 - 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Question 1 :** What is the count of responders per family income range (show all)?\n",
    "+ Create dataframe **df_family_income**, use it to decode Family Income Range\n",
    "+ Select only required fields from *df_Master* for a faster runtime\n",
    "+ Join the dataframes and generate the result\n",
    "+ Please note - *there are null values in the outcome of this join based on findings from deeper analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Count of Responders per Family Income Range\n",
      "+-------------------+-------------------+\n",
      "|FAMILY_INCOME_RANGE|COUNT_OF_RESPONDERS|\n",
      "+-------------------+-------------------+\n",
      "|   LESS THAN $5,000|             33,315|\n",
      "|    12,00 TO 14,999|             20,408|\n",
      "|   15,000 TO 19,999|             20,222|\n",
      "|   10,000 TO 12,999|             19,718|\n",
      "|     7,500 TO 9,999|             15,719|\n",
      "|     5,000 TO 7,499|             11,596|\n",
      "|      30,000 TO 34,|              6,743|\n",
      "|               NULL|              6,620|\n",
      "|   20,000 TO 24,999|              6,312|\n",
      "|   25,000 TO 29,999|              5,803|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load schema and data for family income data structure\n",
    "from schema import family_income_range_schema, family_income_range_data\n",
    "\n",
    "df_family_income = spark.createDataFrame(family_income_range_data, family_income_range_schema)\n",
    "\n",
    "# Create the join and run the analysis\n",
    "question_1 = df_master.select('family_income_range_code')\\\n",
    "    .join(df_family_income, df_master.family_income_range_code == df_family_income.family_code, 'left')\\\n",
    "        .groupBy('family_income_range').count().orderBy('count', ascending = False)\n",
    "\n",
    "# Format and display the result, include count of available records\n",
    "question_1 = question_1.withColumn('count', format_number('count', 0))\n",
    "num_of_records = question_1.count()\n",
    "\n",
    "print(f'\\n\\nThe Count of Responders per Family Income Range')\n",
    "question_1.toDF('FAMILY_INCOME_RANGE','COUNT_OF_RESPONDERS').show(num_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 2 :** What is the count of responders per geographical division/location and race (show top 10)?\n",
    "+ Create dataframe **df_geo_location** and **df_race** to decode Location and Race\n",
    "+ Select only required fields from *df_Master* for a faster execution time \n",
    "+ Join the dataframes and generate the result\n",
    "+ Observation - *there are null values in the outcome of this join based on missing ref values discovered during analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Count of Top 10 Responders per Geographical_Division_Location and Race\n",
      "+------------------+--------+-------------------+\n",
      "| DIVISION_LOCATION|    RACE|COUNT_OF_RESPONDERS|\n",
      "+------------------+--------+-------------------+\n",
      "|    SOUTH ATLANTIC|    NULL|             27,609|\n",
      "|           PACIFIC|    NULL|             20,659|\n",
      "|          MOUNTAIN|    NULL|             18,470|\n",
      "|WEST SOUTH CENTRAL|    NULL|             16,498|\n",
      "|EAST NORTH CENTRAL|    NULL|             15,296|\n",
      "|WEST NORTH CENTRAL|    NULL|             13,052|\n",
      "|   MIDDLE ATLANTIC|    NULL|             12,756|\n",
      "|       NEW ENGLAND|    NULL|             11,281|\n",
      "|EAST SOUTH CENTRAL|    NULL|             10,345|\n",
      "|           PACIFIC|Asian-HP|                 70|\n",
      "+------------------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from schema import division_location_schema, race_schema, division_location_data, race_data\n",
    "\n",
    "df_div_location = spark.createDataFrame(division_location_data, division_location_schema)\n",
    "df_race = spark.createDataFrame(race_data, race_schema)\n",
    "\n",
    "# Create the join and run the analysis\n",
    "question_2 = df_master.select('division_location_code','race_code')\\\n",
    "    .join(df_div_location, df_master.division_location_code == df_div_location.div_loc_code, 'left')\\\n",
    "        .join(df_race, df_master.race_code == df_race.race_code, 'left')\\\n",
    "            .groupBy('division_location','race').count().orderBy('count', ascending = False)\n",
    "\n",
    "# Format and display Top 10 only\n",
    "question_2 = question_2.withColumn('count', format_number('count',0))\n",
    "\n",
    "print(f'\\n\\nThe Count of Top 10 Responders per Geographical_Division_Location and Race')\n",
    "question_2.toDF('DIVISION_LOCATION', 'RACE', 'COUNT_OF_RESPONDERS').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 3 :** How many responders do not have telephone in their house, but can access a telephone elsewhere and telephone interview is accepted?\n",
    "+ Use *df_Master* for fast execution, no decoding is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer to Question (3) is : 635\n"
     ]
    }
   ],
   "source": [
    "question_3 = df_master.where(\n",
    "    (col('household_has_telephone_code') == lit('2')) &\n",
    "    (col('household_can_access_telephone_code')  == lit('1')) &\n",
    "    (col('is_telephone_interview_acceptable_code') == lit('1'))\n",
    ").count()\n",
    "\n",
    "print(f'\\nThe answer to Question (3) is : {question_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 4 :** How many responders can access a telephone, but telephone interview is not accepted?\n",
    "+ Select only required fields from *df_Master* for fast execution time \n",
    "+ Observation - data values for '*Is telephone interview acceptable*' shows values (0,1) instaed of (1,2) as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer to Question (4) is : 0\n"
     ]
    }
   ],
   "source": [
    "question_4 = df_master.where(\n",
    "        (col('household_can_access_telephone_code')  == lit('1')) &\n",
    "        (col('is_telephone_interview_acceptable_code') == lit('2'))\n",
    "    ).count()\n",
    "\n",
    "print(f'\\nThe answer to Question (4) is : {question_4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Project Folder\n",
    "+ The unzipped file is so large (128MB), so let us delete it since we have it's zipped version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dec17pub.dat has been deleted succesfully\n"
     ]
    }
   ],
   "source": [
    "file_path.unlink()\n",
    "print(f'\\n{file_name} has been deleted succesfully' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
